{"cells":[{"cell_type":"markdown","metadata":{"id":"E7a7WuSCGBiS"},"source":["# CHOWDER"]},{"cell_type":"markdown","metadata":{"id":"3WTUvo2GGELp"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14213,"status":"ok","timestamp":1699043494907,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"B7K98CEankGG","outputId":"d5d6c76a-412d-48a1-995f-809ca768b61c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRdeZmxRF6lv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":605,"status":"ok","timestamp":1699043508938,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"_Ym0jJzmGMwX","outputId":"efec4e6e-4bb4-4e5c-8798-a4f496b037fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"GEP9wThKGNmv"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RA9sRTmAGPbH"},"outputs":[],"source":["processed_data_path = '/content/drive/My Drive/Breast_Cancer_Detection/Processed_Data/'\n","\n","X_dev = np.load(processed_data_path + 'X_dev.npy')\n","y_dev = np.load(processed_data_path + 'y_dev.npy')\n","X_zoom = np.load(processed_data_path + 'zoom_train.npy')\n","X_coordinates = np.load(processed_data_path + 'coordinates_dev.npy')\n","\n","X_test = np.load(processed_data_path + 'X_test.npy')\n","X_zoom_test = np.load(processed_data_path + 'zoom_test.npy')\n","X_coordinates_test = np.load(processed_data_path + 'coordinates_test.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIroNyy7DnAq"},"outputs":[],"source":["X_development = np.concatenate((X_dev, X_zoom, X_coordinates), axis=1)\n","X_test = np.concatenate((X_test, X_zoom_test, X_coordinates_test), axis=1)\n","\n","moco_features = X_dev.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6iYS6mEF33-"},"outputs":[],"source":["# Free Memory\n","del X_dev\n","del X_zoom\n","del X_coordinates\n","del X_zoom_test\n","del X_coordinates_test"]},{"cell_type":"markdown","metadata":{"id":"9UZ5pixlHaP_"},"source":["## Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvD3AJlWHbl6"},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(X_development, y_dev, test_size=0.15, stratify=y_dev, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLiPnRYIJxje"},"outputs":[],"source":["# Free up memory\n","del X_development\n","del y_dev"]},{"cell_type":"markdown","metadata":{"id":"kCQyfA4KGiJT"},"source":["## Standardize Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rrujd_x7GmCJ"},"outputs":[],"source":["def X_standardize(X_train, X_val, X_test, moco_features):\n","\n","    # MoCo Features\n","    moco_train = X_train[:, :moco_features]\n","    moco_val = X_val[:, :moco_features]\n","    moco_test = X_test[:, :moco_features]\n","\n","    moco_mean = np.mean(moco_train)\n","    moco_std = np.std(moco_train)\n","\n","    moco_train_scaled = (moco_train - moco_mean) / moco_std\n","    moco_val_scaled = (moco_val - moco_mean) / moco_std\n","    moco_test_scaled = (moco_test - moco_mean) / moco_std\n","\n","    del moco_train\n","    del moco_val\n","    del moco_test\n","\n","    # Zoom\n","    zoom_train = X_train[:, moco_features:moco_features+1000]\n","    zoom_val = X_val[:, moco_features:moco_features+1000]\n","    zoom_test = X_test[:, moco_features:moco_features+1000]\n","\n","    zoom_mean = np.mean(zoom_train)\n","    zoom_std = np.std(zoom_train)\n","\n","    zoom_train_scaled = (zoom_train - zoom_mean) / zoom_std\n","    zoom_val_scaled = (zoom_val - zoom_mean) / zoom_std\n","    zoom_test_scaled = (zoom_test - zoom_mean) / zoom_std\n","\n","    del zoom_train\n","    del zoom_val\n","    del zoom_test\n","\n","    # Merge Data\n","    X_train_scaled = np.concatenate((moco_train_scaled, zoom_train_scaled, X_train[:, moco_features+1000:]), axis=1)\n","    X_val_scaled = np.concatenate((moco_val_scaled, zoom_val_scaled, X_val[:, moco_features+1000:]), axis=1)\n","    X_test_scaled = np.concatenate((moco_test_scaled, zoom_test_scaled, X_test[:, moco_features+1000:]), axis=1)\n","\n","    return X_train_scaled, X_val_scaled, X_test_scaled"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r1BHVpzHwEP"},"outputs":[],"source":["X_train_scaled, X_val_scaled, X_test_scaled = X_standardize(X_train, X_val, X_test, moco_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OoWbwHJzJ6mZ"},"outputs":[],"source":["# Free Memory\n","del X_train\n","del X_val\n","del X_test"]},{"cell_type":"markdown","metadata":{"id":"ypElU9wJJO2X"},"source":["## Convert to Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHP07ms3I22G"},"outputs":[],"source":["X_train_tensor = torch.Tensor(X_train_scaled)\n","X_val_tensor = torch.Tensor(X_val_scaled)\n","X_test_tensor = torch.Tensor(X_test_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FLD-9qYJTJ9"},"outputs":[],"source":["# Delete redundant variables to free up memory\n","del X_test_scaled\n","del X_train_scaled\n","del X_val_scaled"]},{"cell_type":"markdown","metadata":{"id":"lvkzmqLUJ9o3"},"source":["## CHOWDER Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs8Yoh9SJ_wP"},"outputs":[],"source":["class CHOWDER(nn.Module):\n","\n","    def __init__(self):\n","        super(CHOWDER, self).__init__()\n","\n","        # Convolutional layer\n","        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2048, stride=2048)\n","\n","        self.fc1 = nn.Linear(12, 1)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","\n","        # Set std to the square root of the number of edges\n","        std_conv = 2048**(-0.5)\n","        # Initialize weights with random normal values for the convolutional layer\n","        nn.init.normal_(self.conv.weight, mean=0.0, std=std_conv)\n","        nn.init.constant_(self.conv.bias, 0)\n","\n","\n","    def forward(self, x):\n","\n","        # Seperate x into moco data and metadata\n","        X_moco = x[:, :, :2048000]\n","        X_zoom = x[:, :, 2048000:2048000+1000]\n","        X_coord = x[:, :, 2048000+1000:]\n","\n","\n","        ## CONVOLUTION LAYER\n","\n","        conv_output = self.conv(X_moco)\n","\n","        # Calculate L2-norm on weights in the convolutional layer\n","        l2_reg_conv = 0.0\n","        for param in self.conv.parameters():\n","            l2_reg_conv += torch.sum(param ** 2)\n","\n","\n","        ## MINMAX LAYER\n","\n","        # Sort each row of the conv layer (each row is a sample)\n","        sorted_output, sorted_indices = torch.sort(conv_output, dim=2)\n","\n","        # Number of top instances and negative evidence\n","        R = 2\n","\n","        # Select the first two and last two sorted tiles\n","        selected_output = sorted_output[:, :, :R]\n","        selected_tiles = torch.cat((selected_output, sorted_output[:, :, -R:]), dim=2)\n","\n","        # Get Coord\n","\n","        selected_indices = sorted_indices[:, :, :R]\n","        selected_coord_top_1 = torch.gather(X_coord, 2, 2*selected_indices)\n","        selected_coord_top_2 = torch.gather(X_coord, 2, 2*selected_indices+1)\n","        selected_coord_top = torch.cat((selected_coord_top_1, selected_coord_top_2), dim=2)\n","\n","        selected_indices = sorted_indices[:, :, -R:]\n","        selected_coord_bottom_1 = torch.gather(X_coord, 2, 2*selected_indices)\n","        selected_coord_bottom_2 = torch.gather(X_coord, 2, 2*selected_indices+1)\n","        selected_coord_bottom = torch.cat((selected_coord_bottom_1, selected_coord_bottom_2), dim=2)\n","\n","        # Concatenate the top and bottom zoom values\n","        selected_meta = torch.cat((selected_coord_top, selected_coord_bottom), dim=2)\n","\n","        # Join tiles and zoom\n","        a0 = torch.cat((selected_tiles, selected_meta), dim=2)\n","        a0 = a0.squeeze()\n","\n","        # Linear layer\n","        z1 = self.fc1(a0)\n","\n","        # Nonlinear layer\n","        output = torch.sigmoid(z1)\n","\n","        return output, l2_reg_conv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699043545555,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"LfK_D8ki0_8a","outputId":"c3bde4b7-dd11-43de-fc0d-ccb23a9db789"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CHOWDER(\n","  (conv): Conv1d(1, 1, kernel_size=(2048,), stride=(2048,))\n","  (fc1): Linear(in_features=12, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":15}],"source":["demo_model = CHOWDER()\n","\n","demo_model"]},{"cell_type":"markdown","metadata":{"id":"k9FI8PUMSAHW"},"source":["## Setup Hyperparameters and Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwA77N-8SDvp"},"outputs":[],"source":["# Define hyperparameters\n","BATCH_SIZE = 10\n","LR = 0.001\n","EPOCHS = 20\n","NUM_ENSEMBLE_MODELS = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHLHzyStTH9S"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, data_tensor, target_tensor):\n","        self.data = data_tensor\n","        self.target = target_tensor\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.target[index]\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","# Create custom datasets\n","train_dataset = CustomDataset(X_train_tensor, torch.Tensor(y_train))\n","val_dataset = CustomDataset(X_val_tensor, torch.Tensor(y_val))\n","\n","# Create DataLoader\n","train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pusa-2_1Tbs5"},"outputs":[],"source":["# Free Up Memory\n","del X_train_tensor\n","del X_val_tensor\n","del y_train\n","del y_val"]},{"cell_type":"markdown","metadata":{"id":"d3_KACwOkeNm"},"source":["## Train Model and Validate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_X_vyiqBkdj-","outputId":"f2bdfe7b-7147-4f91-8621-c13a0031593c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," ----- Model 1 -----\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/20] - Train Loss: 55.2415 Train AUC Score: 0.5117         Val Loss: 55.4521 Val AUC Score: 0.4782\n","Epoch [2/20] - Train Loss: 44.0705 Train AUC Score: 0.4769         Val Loss: 33.6679 Val AUC Score: 0.4907\n","Epoch [3/20] - Train Loss: 25.7124 Train AUC Score: 0.4823         Val Loss: 16.5836 Val AUC Score: 0.4215\n","Epoch [4/20] - Train Loss: 14.9620 Train AUC Score: 0.4956         Val Loss: 8.3711 Val AUC Score: 0.5842\n","Epoch [5/20] - Train Loss: 13.7194 Train AUC Score: 0.5264         Val Loss: 9.3522 Val AUC Score: 0.5055\n","Epoch [6/20] - Train Loss: 13.0617 Train AUC Score: 0.4770         Val Loss: 5.7990 Val AUC Score: 0.5906\n","Epoch [7/20] - Train Loss: 11.3443 Train AUC Score: 0.4819         Val Loss: 8.4215 Val AUC Score: 0.5365\n","Epoch [8/20] - Train Loss: 12.8292 Train AUC Score: 0.4563         Val Loss: 8.3744 Val AUC Score: 0.5915\n","Epoch [9/20] - Train Loss: 10.8507 Train AUC Score: 0.3848         Val Loss: 6.3261 Val AUC Score: 0.6607\n","Epoch [10/20] - Train Loss: 9.5353 Train AUC Score: 0.4465         Val Loss: 4.8872 Val AUC Score: 0.5962\n","Epoch [11/20] - Train Loss: 10.5229 Train AUC Score: 0.4626         Val Loss: 5.5377 Val AUC Score: 0.6697\n","Epoch [12/20] - Train Loss: 8.5774 Train AUC Score: 0.4826         Val Loss: 9.2571 Val AUC Score: 0.4020\n","Epoch [13/20] - Train Loss: 8.6133 Train AUC Score: 0.4792         Val Loss: 10.9441 Val AUC Score: 0.5540\n","Epoch [14/20] - Train Loss: 8.2908 Train AUC Score: 0.4559         Val Loss: 6.8518 Val AUC Score: 0.3750\n","Epoch [15/20] - Train Loss: 7.5000 Train AUC Score: 0.4751         Val Loss: 8.8161 Val AUC Score: 0.3923\n","Epoch [16/20] - Train Loss: 6.8952 Train AUC Score: 0.4681         Val Loss: 2.6651 Val AUC Score: 0.5381\n","Epoch [17/20] - Train Loss: 5.7621 Train AUC Score: 0.5240         Val Loss: 2.9788 Val AUC Score: 0.5987\n","Epoch [18/20] - Train Loss: 6.1469 Train AUC Score: 0.5462         Val Loss: 3.3749 Val AUC Score: 0.5156\n","Epoch [19/20] - Train Loss: 4.9170 Train AUC Score: 0.4977         Val Loss: 5.4538 Val AUC Score: 0.5403\n","Epoch [20/20] - Train Loss: 3.5575 Train AUC Score: 0.6384         Val Loss: 5.0803 Val AUC Score: 0.5365\n","\n"," ----- Model 2 -----\n","Epoch [1/20] - Train Loss: 24.6672 Train AUC Score: 0.4378         Val Loss: 19.5671 Val AUC Score: 0.4941\n","Epoch [2/20] - Train Loss: 20.7478 Train AUC Score: 0.4834         Val Loss: 20.1994 Val AUC Score: 0.6035\n","Epoch [3/20] - Train Loss: 14.5896 Train AUC Score: 0.5075         Val Loss: 11.4474 Val AUC Score: 0.4281\n","Epoch [4/20] - Train Loss: 12.3905 Train AUC Score: 0.4965         Val Loss: 13.6165 Val AUC Score: 0.5657\n","Epoch [5/20] - Train Loss: 14.4878 Train AUC Score: 0.5037         Val Loss: 18.9202 Val AUC Score: 0.4375\n","Epoch [6/20] - Train Loss: 11.8112 Train AUC Score: 0.5329         Val Loss: 13.9673 Val AUC Score: 0.3033\n","Epoch [7/20] - Train Loss: 9.9586 Train AUC Score: 0.5319         Val Loss: 6.1087 Val AUC Score: 0.4696\n","Epoch [8/20] - Train Loss: 11.7298 Train AUC Score: 0.5174         Val Loss: 7.8479 Val AUC Score: 0.3512\n","Epoch [9/20] - Train Loss: 10.4060 Train AUC Score: 0.5243         Val Loss: 8.7989 Val AUC Score: 0.3936\n","Epoch [10/20] - Train Loss: 10.5640 Train AUC Score: 0.4913         Val Loss: 8.2280 Val AUC Score: 0.5758\n","Epoch [11/20] - Train Loss: 8.6514 Train AUC Score: 0.4537         Val Loss: 9.9049 Val AUC Score: 0.4777\n","Epoch [12/20] - Train Loss: 6.8013 Train AUC Score: 0.5715         Val Loss: 9.2442 Val AUC Score: 0.3219\n","Epoch [13/20] - Train Loss: 7.0919 Train AUC Score: 0.5995         Val Loss: 4.9887 Val AUC Score: 0.3929\n","Epoch [14/20] - Train Loss: 5.3733 Train AUC Score: 0.6750         Val Loss: 7.3457 Val AUC Score: 0.4583\n","Epoch [15/20] - Train Loss: 6.3963 Train AUC Score: 0.6984         Val Loss: 7.3494 Val AUC Score: 0.4167\n","Epoch [16/20] - Train Loss: 5.8480 Train AUC Score: 0.6214         Val Loss: 4.1817 Val AUC Score: 0.5134\n","Epoch [17/20] - Train Loss: 4.0948 Train AUC Score: 0.6877         Val Loss: 4.1933 Val AUC Score: 0.4918\n","Epoch [18/20] - Train Loss: 3.5837 Train AUC Score: 0.6775         Val Loss: 5.9348 Val AUC Score: 0.2852\n","Epoch [19/20] - Train Loss: 3.7884 Train AUC Score: 0.7063         Val Loss: 13.5563 Val AUC Score: 0.2910\n","Epoch [20/20] - Train Loss: 5.1293 Train AUC Score: 0.6490         Val Loss: 6.5861 Val AUC Score: 0.5124\n","\n"," ----- Model 3 -----\n","Epoch [1/20] - Train Loss: 8.8479 Train AUC Score: 0.5158         Val Loss: 9.7887 Val AUC Score: 0.5110\n","Epoch [2/20] - Train Loss: 9.6447 Train AUC Score: 0.5191         Val Loss: 6.2976 Val AUC Score: 0.6199\n","Epoch [3/20] - Train Loss: 7.7093 Train AUC Score: 0.5156         Val Loss: 6.1451 Val AUC Score: 0.6279\n","Epoch [4/20] - Train Loss: 6.3846 Train AUC Score: 0.4961         Val Loss: 8.2149 Val AUC Score: 0.6139\n","Epoch [5/20] - Train Loss: 7.7240 Train AUC Score: 0.5493         Val Loss: 8.3619 Val AUC Score: 0.6535\n","Epoch [6/20] - Train Loss: 7.3610 Train AUC Score: 0.5419         Val Loss: 5.4896 Val AUC Score: 0.6562\n","Epoch [7/20] - Train Loss: 7.9140 Train AUC Score: 0.5080         Val Loss: 5.1745 Val AUC Score: 0.6362\n","Epoch [8/20] - Train Loss: 7.5557 Train AUC Score: 0.5102         Val Loss: 10.6352 Val AUC Score: 0.6075\n","Epoch [9/20] - Train Loss: 6.8502 Train AUC Score: 0.5274         Val Loss: 7.7397 Val AUC Score: 0.6009\n","Epoch [10/20] - Train Loss: 5.4149 Train AUC Score: 0.4616         Val Loss: 7.2743 Val AUC Score: 0.6042\n","Epoch [11/20] - Train Loss: 6.7147 Train AUC Score: 0.5261         Val Loss: 11.9858 Val AUC Score: 0.5844\n","Epoch [12/20] - Train Loss: 4.9342 Train AUC Score: 0.5528         Val Loss: 8.7068 Val AUC Score: 0.6875\n","Epoch [13/20] - Train Loss: 5.9675 Train AUC Score: 0.4658         Val Loss: 8.7462 Val AUC Score: 0.6791\n","Epoch [14/20] - Train Loss: 5.3564 Train AUC Score: 0.4958         Val Loss: 6.8841 Val AUC Score: 0.6429\n","Epoch [15/20] - Train Loss: 4.0848 Train AUC Score: 0.5445         Val Loss: 14.7549 Val AUC Score: 0.4479\n","Epoch [16/20] - Train Loss: 4.3956 Train AUC Score: 0.4497         Val Loss: 5.3634 Val AUC Score: 0.4565\n","Epoch [17/20] - Train Loss: 3.2137 Train AUC Score: 0.5128         Val Loss: 3.8247 Val AUC Score: 0.7455\n","Epoch [18/20] - Train Loss: 3.0172 Train AUC Score: 0.5582         Val Loss: 4.7029 Val AUC Score: 0.4792\n","Epoch [19/20] - Train Loss: 2.9826 Train AUC Score: 0.5478         Val Loss: 4.6644 Val AUC Score: 0.5357\n","Epoch [20/20] - Train Loss: 2.2000 Train AUC Score: 0.5705         Val Loss: 1.7984 Val AUC Score: 0.7173\n","\n"," ----- Model 4 -----\n","Epoch [1/20] - Train Loss: 52.6492 Train AUC Score: 0.4948         Val Loss: 41.8829 Val AUC Score: 0.5379\n","Epoch [2/20] - Train Loss: 41.7846 Train AUC Score: 0.4847         Val Loss: 26.4639 Val AUC Score: 0.5990\n","Epoch [3/20] - Train Loss: 22.1376 Train AUC Score: 0.5832         Val Loss: 16.4016 Val AUC Score: 0.5515\n","Epoch [4/20] - Train Loss: 16.0206 Train AUC Score: 0.5335         Val Loss: 13.7604 Val AUC Score: 0.5527\n","Epoch [5/20] - Train Loss: 14.5510 Train AUC Score: 0.5485         Val Loss: 16.3203 Val AUC Score: 0.6302\n","Epoch [6/20] - Train Loss: 14.9498 Train AUC Score: 0.5202         Val Loss: 18.4274 Val AUC Score: 0.5344\n","Epoch [7/20] - Train Loss: 14.5977 Train AUC Score: 0.5184         Val Loss: 18.8897 Val AUC Score: 0.5637\n","Epoch [8/20] - Train Loss: 12.3116 Train AUC Score: 0.4914         Val Loss: 19.1820 Val AUC Score: 0.5221\n","Epoch [9/20] - Train Loss: 13.7916 Train AUC Score: 0.5104         Val Loss: 16.3677 Val AUC Score: 0.5764\n","Epoch [10/20] - Train Loss: 14.0853 Train AUC Score: 0.5340         Val Loss: 13.1332 Val AUC Score: 0.5569\n","Epoch [11/20] - Train Loss: 11.3321 Train AUC Score: 0.5760         Val Loss: 19.1996 Val AUC Score: 0.5126\n","Epoch [12/20] - Train Loss: 11.0499 Train AUC Score: 0.5195         Val Loss: 17.8019 Val AUC Score: 0.5156\n","Epoch [13/20] - Train Loss: 10.5458 Train AUC Score: 0.5803         Val Loss: 15.2321 Val AUC Score: 0.5599\n","Epoch [14/20] - Train Loss: 10.6426 Train AUC Score: 0.5380         Val Loss: 12.9200 Val AUC Score: 0.5312\n","Epoch [15/20] - Train Loss: 10.0937 Train AUC Score: 0.5297         Val Loss: 12.2929 Val AUC Score: 0.5894\n","Epoch [16/20] - Train Loss: 8.5041 Train AUC Score: 0.5528         Val Loss: 11.6496 Val AUC Score: 0.6340\n","Epoch [17/20] - Train Loss: 7.0197 Train AUC Score: 0.5453         Val Loss: 10.0984 Val AUC Score: 0.4425\n","Epoch [18/20] - Train Loss: 6.9006 Train AUC Score: 0.5242         Val Loss: 8.1149 Val AUC Score: 0.7123\n","Epoch [19/20] - Train Loss: 5.8666 Train AUC Score: 0.4898         Val Loss: 5.5636 Val AUC Score: 0.4933\n","Epoch [20/20] - Train Loss: 4.5752 Train AUC Score: 0.5642         Val Loss: 2.9817 Val AUC Score: 0.5372\n","\n"," ----- Model 5 -----\n","Epoch [1/20] - Train Loss: 16.9903 Train AUC Score: 0.5105         Val Loss: 14.0092 Val AUC Score: 0.6637\n","Epoch [2/20] - Train Loss: 14.8730 Train AUC Score: 0.4518         Val Loss: 13.3099 Val AUC Score: 0.6354\n","Epoch [3/20] - Train Loss: 15.2276 Train AUC Score: 0.3995         Val Loss: 13.8969 Val AUC Score: 0.7361\n","Epoch [4/20] - Train Loss: 13.0159 Train AUC Score: 0.4798         Val Loss: 16.7083 Val AUC Score: 0.5223\n","Epoch [5/20] - Train Loss: 13.0584 Train AUC Score: 0.5225         Val Loss: 14.7055 Val AUC Score: 0.6320\n","Epoch [6/20] - Train Loss: 15.8392 Train AUC Score: 0.5390         Val Loss: 22.4824 Val AUC Score: 0.3847\n","Epoch [7/20] - Train Loss: 16.3705 Train AUC Score: 0.5031         Val Loss: 18.2346 Val AUC Score: 0.6850\n","Epoch [8/20] - Train Loss: 20.7407 Train AUC Score: 0.5572         Val Loss: 10.2635 Val AUC Score: 0.6632\n","Epoch [9/20] - Train Loss: 15.7709 Train AUC Score: 0.5284         Val Loss: 15.8959 Val AUC Score: 0.5948\n","Epoch [10/20] - Train Loss: 17.4285 Train AUC Score: 0.5192         Val Loss: 18.5455 Val AUC Score: 0.6010\n","Epoch [11/20] - Train Loss: 16.4413 Train AUC Score: 0.5275         Val Loss: 15.8779 Val AUC Score: 0.5391\n","Epoch [12/20] - Train Loss: 15.4508 Train AUC Score: 0.4743         Val Loss: 12.1681 Val AUC Score: 0.6435\n","Epoch [13/20] - Train Loss: 14.1406 Train AUC Score: 0.5976         Val Loss: 12.3234 Val AUC Score: 0.6972\n","Epoch [14/20] - Train Loss: 13.1398 Train AUC Score: 0.6248         Val Loss: 14.3082 Val AUC Score: 0.7403\n","Epoch [15/20] - Train Loss: 15.7064 Train AUC Score: 0.5951         Val Loss: 15.5348 Val AUC Score: 0.5503\n","Epoch [16/20] - Train Loss: 15.3744 Train AUC Score: 0.5470         Val Loss: 18.6107 Val AUC Score: 0.5429\n","Epoch [17/20] - Train Loss: 13.9316 Train AUC Score: 0.6521         Val Loss: 6.0318 Val AUC Score: 0.5112\n","Epoch [18/20] - Train Loss: 11.3929 Train AUC Score: 0.6337         Val Loss: 8.3447 Val AUC Score: 0.5924\n","Epoch [19/20] - Train Loss: 13.4740 Train AUC Score: 0.6279         Val Loss: 15.4665 Val AUC Score: 0.5998\n","Epoch [20/20] - Train Loss: 13.6229 Train AUC Score: 0.5944         Val Loss: 17.9028 Val AUC Score: 0.4973\n","\n"," ----- Model 6 -----\n","Epoch [1/20] - Train Loss: 17.7271 Train AUC Score: 0.5082         Val Loss: 24.5466 Val AUC Score: 0.5818\n","Epoch [2/20] - Train Loss: 19.6080 Train AUC Score: 0.5226         Val Loss: 25.3893 Val AUC Score: 0.5590\n","Epoch [3/20] - Train Loss: 20.0822 Train AUC Score: 0.5110         Val Loss: 21.7943 Val AUC Score: 0.4081\n","Epoch [4/20] - Train Loss: 17.0845 Train AUC Score: 0.4614         Val Loss: 25.2398 Val AUC Score: 0.4022\n","Epoch [5/20] - Train Loss: 17.8084 Train AUC Score: 0.4974         Val Loss: 16.1885 Val AUC Score: 0.6079\n","Epoch [6/20] - Train Loss: 15.0915 Train AUC Score: 0.5353         Val Loss: 11.8082 Val AUC Score: 0.6101\n","Epoch [7/20] - Train Loss: 16.0425 Train AUC Score: 0.4925         Val Loss: 12.0715 Val AUC Score: 0.6815\n","Epoch [8/20] - Train Loss: 11.8288 Train AUC Score: 0.5544         Val Loss: 12.6853 Val AUC Score: 0.5883\n","Epoch [9/20] - Train Loss: 10.4049 Train AUC Score: 0.6026         Val Loss: 13.8860 Val AUC Score: 0.5417\n","Epoch [10/20] - Train Loss: 11.1240 Train AUC Score: 0.5568         Val Loss: 16.3143 Val AUC Score: 0.5469\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/20] - Train Loss: 10.9780 Train AUC Score: 0.5093         Val Loss: 17.2679 Val AUC Score: 0.4573\n","Epoch [12/20] - Train Loss: 10.7863 Train AUC Score: 0.5281         Val Loss: 12.2574 Val AUC Score: 0.4970\n","Epoch [13/20] - Train Loss: 10.9574 Train AUC Score: 0.5872         Val Loss: 13.4124 Val AUC Score: 0.4623\n","Epoch [14/20] - Train Loss: 9.1554 Train AUC Score: 0.5925         Val Loss: 11.6825 Val AUC Score: 0.5312\n","Epoch [15/20] - Train Loss: 8.9841 Train AUC Score: 0.6130         Val Loss: 7.3253 Val AUC Score: 0.5386\n","Epoch [16/20] - Train Loss: 10.3072 Train AUC Score: 0.5624         Val Loss: 10.8647 Val AUC Score: 0.5885\n","Epoch [17/20] - Train Loss: 6.9680 Train AUC Score: 0.6258         Val Loss: 5.9549 Val AUC Score: 0.3958\n","Epoch [18/20] - Train Loss: 7.3632 Train AUC Score: 0.5991         Val Loss: 12.3221 Val AUC Score: 0.4410\n","Epoch [19/20] - Train Loss: 7.4774 Train AUC Score: 0.5899         Val Loss: 5.1559 Val AUC Score: 0.4861\n","Epoch [20/20] - Train Loss: 7.5094 Train AUC Score: 0.6314         Val Loss: 6.9198 Val AUC Score: 0.4372\n","\n"," ----- Model 7 -----\n","Epoch [1/20] - Train Loss: 21.9622 Train AUC Score: 0.4901         Val Loss: 17.5529 Val AUC Score: 0.5640\n","Epoch [2/20] - Train Loss: 17.0107 Train AUC Score: 0.5003         Val Loss: 15.1735 Val AUC Score: 0.4715\n","Epoch [3/20] - Train Loss: 9.1450 Train AUC Score: 0.4746         Val Loss: 15.1633 Val AUC Score: 0.3875\n","Epoch [4/20] - Train Loss: 9.2193 Train AUC Score: 0.5164         Val Loss: 10.2705 Val AUC Score: 0.3769\n","Epoch [5/20] - Train Loss: 5.2955 Train AUC Score: 0.5079         Val Loss: 7.3719 Val AUC Score: 0.5025\n","Epoch [6/20] - Train Loss: 5.2670 Train AUC Score: 0.5257         Val Loss: 7.8100 Val AUC Score: 0.3983\n","Epoch [7/20] - Train Loss: 6.3568 Train AUC Score: 0.5046         Val Loss: 9.2802 Val AUC Score: 0.3356\n","Epoch [8/20] - Train Loss: 4.4520 Train AUC Score: 0.4705         Val Loss: 8.4301 Val AUC Score: 0.5833\n","Epoch [9/20] - Train Loss: 3.8307 Train AUC Score: 0.5584         Val Loss: 7.7504 Val AUC Score: 0.5245\n","Epoch [10/20] - Train Loss: 4.0793 Train AUC Score: 0.5280         Val Loss: 7.0008 Val AUC Score: 0.5238\n","Epoch [11/20] - Train Loss: 4.5165 Train AUC Score: 0.5111         Val Loss: 5.0400 Val AUC Score: 0.4375\n","Epoch [12/20] - Train Loss: 3.5783 Train AUC Score: 0.5472         Val Loss: 4.6467 Val AUC Score: 0.4866\n","Epoch [13/20] - Train Loss: 2.5770 Train AUC Score: 0.5778         Val Loss: 2.0975 Val AUC Score: 0.5123\n","Epoch [14/20] - Train Loss: 2.4029 Train AUC Score: 0.6035         Val Loss: 1.3184 Val AUC Score: 0.7902\n","Epoch [15/20] - Train Loss: 2.0168 Train AUC Score: 0.5859         Val Loss: 1.7494 Val AUC Score: 0.5937\n","Epoch [16/20] - Train Loss: 2.1017 Train AUC Score: 0.5364         Val Loss: 1.5818 Val AUC Score: 0.7208\n"]}],"source":["# Define loss function and optimizer\n","loss_function = nn.BCELoss()\n","\n","# Create an ensemble of models\n","ensemble_models = []\n","\n","for _ in range(NUM_ENSEMBLE_MODELS):\n","\n","    # Initialize the model\n","    model = CHOWDER()\n","\n","    # Define optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","    ensemble_models.append((model, optimizer))\n","\n","# Setup model counter\n","model_counter = 1\n","\n","# Training and validation loops for each model in the ensemble\n","for model, optimizer in ensemble_models:\n","\n","    print(f'\\n ----- Model {model_counter} -----')\n","    model_counter += 1\n","\n","    # Loop thorugh each epoch\n","    for epoch in range(EPOCHS):\n","\n","        ## Training loop\n","\n","        # Put model in train mode\n","        model.train()\n","\n","        # Initialize loss, AUC and count\n","        total_loss = 0.0\n","        auroc_hist_train = 0.0\n","        total_count = 0.0\n","\n","        # Train in batches\n","        for batch_x, batch_y in train_dl:\n","\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # Make predictions and get L2-norm from conv layer and linear layer\n","            pred, L2_conv = model(batch_x_transformed)\n","\n","            # Calculate loss\n","            batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","            loss = loss_function(pred, batch_y) + (0.1*L2_conv)\n","\n","            # Calculate gradients\n","            loss.backward()\n","\n","            # Make step in gradient descent\n","            optimizer.step()\n","\n","            # Add to loss counter for this epoch\n","            total_loss += loss.item() * len(batch_y)\n","            total_count += len(batch_y)\n","\n","            # Calculate AUC for batch\n","            auroc_hist_train += torchmetrics.AUROC(task=\"binary\")(pred, batch_y).item() * len(batch_y)\n","\n","        # Calculate loss and AUC per sample\n","        train_average_loss = total_loss / total_count\n","        train_average_auc = auroc_hist_train/ total_count\n","\n","\n","        ## Validation loop\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","\n","        # Fix gradients (only using model to predict)\n","        with torch.no_grad():\n","\n","            # Initialize loss, AUC and count\n","            total_loss = 0.0\n","            auroc_hist_val = 0.0\n","            total_count = 0.0\n","\n","            # Validate in batches\n","            for batch_x, batch_y in val_dl:\n","\n","                batch_x_transformed = batch_x.unsqueeze(1)\n","\n","                # Make predictions\n","                val_pred, _ = model(batch_x_transformed)\n","\n","                # Calculate loss\n","                batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","                loss = loss_function(val_pred, batch_y)\n","\n","                # Add to loss for this epoch\n","                total_loss += loss.item() * len(batch_y)\n","                total_count += len(batch_y)\n","\n","                # Calculate AUC for batch\n","                auroc_hist_val += torchmetrics.AUROC(task=\"binary\")(val_pred, batch_y).item() * len(batch_y)\n","\n","        # Calculate loss and AUC per sample\n","        val_average_loss = total_loss / total_count\n","        val_average_auc =  auroc_hist_val / total_count\n","\n","        # Print results from each epoch\n","        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_average_loss:.4f} Train AUC Score: {train_average_auc:.4f} \\\n","        Val Loss: {val_average_loss:.4f} Val AUC Score: {val_average_auc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLJ3f4HD63Zb"},"outputs":[],"source":["# Define loss function and optimizer\n","loss_function = nn.BCELoss()\n","\n","# Create an ensemble of models\n","ensemble_models = []\n","\n","for _ in range(NUM_ENSEMBLE_MODELS):\n","\n","    # Initialize the model\n","    model = CHOWDER()\n","\n","    # Define optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","    ensemble_models.append((model, optimizer))\n","\n","# Setup model counter\n","model_counter = 1\n","\n","# Training loops for each model in the ensemble\n","for model, optimizer in ensemble_models:\n","\n","    print(f'\\n ----- Model {model_counter} -----')\n","    model_counter += 1\n","\n","    # Loop thorugh each epoch\n","    for epoch in range(EPOCHS):\n","\n","        ## Training loop\n","\n","        # Put model in train mode\n","        model.train()\n","\n","        # Initialize loss, AUC and count\n","        total_loss = 0.0\n","        auroc_hist_train = 0.0\n","        total_count = 0.0\n","\n","        # Train in batches\n","        for batch_x, batch_y in dev_dl:\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Make predictions and get L2-norm from conv layer\n","            pred, L2_term = model(batch_x_transformed)\n","\n","            # Calculate loss\n","            batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","            loss = loss_function(pred, batch_y) + (0.5*L2_term)\n","\n","            # Calculate gradients\n","            loss.backward()\n","\n","            # Make step in gradient descent\n","            optimizer.step()\n","\n","            # Add to loss counter for this epoch\n","            total_loss += loss.item() * len(batch_y)\n","            total_count += len(batch_y)\n","\n","            # Calculate AUC for batch\n","            auroc_hist_train += torchmetrics.AUROC(task=\"binary\")(pred, batch_y).item() * len(batch_y)\n","\n","        # Calculate loss and AUC per sample\n","        train_average_loss = total_loss / total_count\n","        train_average_auc = auroc_hist_train/ total_count\n","\n","        # Print results from each epoch\n","        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_average_loss:.4f} Train AUC Score: {train_average_auc:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"5eCI-fHc1Oeb"},"source":["## Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mU3UREXZ1QH_"},"outputs":[],"source":["## Setup DataLoaders\n","\n","# Create a dummy y for test set\n","y_test_dummy = torch.zeros(len(X_test_tensor),)\n","\n","# Create custom datasets\n","test_dataset = CustomDataset(X_test_tensor, y_test_dummy)\n","\n","# Create DataLoader\n","test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# Merge them using ConcatDataset\n","dev_dataset = ConcatDataset([train_dataset, val_dataset])\n","\n","# Create a DataLoader for the merged dataset\n","dev_dl = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaEiv9s03HQA"},"outputs":[],"source":["# Free Up Memory\n","del X_test_tensor\n","del train_dataset\n","del val_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcMwUt273UCM"},"outputs":[],"source":["# Define loss function and optimizer\n","loss_function = nn.BCELoss()\n","\n","# Create an ensemble of models\n","ensemble_models = []\n","\n","for _ in range(NUM_ENSEMBLE_MODELS):\n","\n","    # Initialize the model\n","    model = CHOWDER()\n","\n","    # Define optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","    ensemble_models.append((model, optimizer))\n","\n","# Setup model counter\n","model_counter = 1\n","\n","# Training loops for each model in the ensemble\n","for model, optimizer in ensemble_models:\n","\n","    print(f'\\n ----- Model {model_counter} -----')\n","    model_counter += 1\n","\n","    # Loop thorugh each epoch\n","    for epoch in range(EPOCHS):\n","\n","        ## Training loop\n","\n","        # Put model in train mode\n","        model.train()\n","\n","        # Initialize loss, AUC and count\n","        total_loss = 0.0\n","        auroc_hist_train = 0.0\n","        total_count = 0.0\n","\n","        # Train in batches\n","        for batch_x, batch_y in dev_dl:\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # Make predictions and get L2-norm from conv layer\n","            pred, l2_reg_conv, l2_reg_linear = model(batch_x_transformed)\n","\n","            # Calculate loss\n","            batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","            loss = loss_function(pred, batch_y) + (0.1*l2_reg_conv) + (0.0001*l2_reg_linear)\n","\n","            # Calculate gradients\n","            loss.backward()\n","\n","            # Make step in gradient descent\n","            optimizer.step()\n","\n","            # Add to loss counter for this epoch\n","            total_loss += loss.item() * len(batch_y)\n","            total_count += len(batch_y)\n","\n","            # Calculate AUC for batch\n","            auroc_hist_train += torchmetrics.AUROC(task=\"binary\")(pred, batch_y).item() * len(batch_y)\n","\n","        # Calculate loss and AUC per sample\n","        train_average_loss = total_loss / total_count\n","        train_average_auc = auroc_hist_train/ total_count\n","\n","        # Print results from each epoch\n","        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_average_loss:.4f} Train AUC Score: {train_average_auc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLCvU2N73s6M"},"outputs":[],"source":["# After training all models, calculate the ensemble AUC\n","\n","# Store predictions and labels\n","ensemble_predictions = np.zeros((len(test_dataset), len(ensemble_models)))\n","\n","# Do not change gradients\n","with torch.no_grad():\n","\n","    model_counter = 0\n","\n","    # Loop through each model\n","    for model, _ in ensemble_models:\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","\n","        # Get predictions on the validation dataset\n","        predictions = np.empty((0, 1))\n","\n","        for batch_x, _ in test_dl:\n","\n","            extra = 10 - len(batch_x)\n","\n","            # If batch size is smaller than 10, pad rows in batch_x with 0s\n","            if extra > 0:\n","                pad_tensor = torch.zeros((extra,) + batch_x.shape[1:], dtype=batch_x.dtype)\n","                batch_x = torch.cat((batch_x, pad_tensor), dim=0)\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Make predictions\n","            pred, _, _ = model(batch_x_transformed)\n","\n","            # Save predictions\n","            pred_numpy = pred.numpy()\n","            predictions = np.concatenate((predictions, pred_numpy), axis=0)\n","\n","            if extra > 0:\n","              predictions = predictions[:-extra]\n","\n","        ensemble_predictions[:, model_counter] = predictions.squeeze()\n","        model_counter += 1\n","\n","# Average the predictions from all models\n","average_prediction = np.mean(ensemble_predictions, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULTV1bdoWvXq"},"outputs":[],"source":["# Load metadata about each sample\n","data_path = '/content/drive/My Drive/Breast_Cancer_Detection/Data/'\n","df_test = pd.read_csv(data_path + \"test_metadata.csv\")\n","\n","# Join sample ID metadata with probability prediction\n","CHOWDER_submission = pd.DataFrame( {\"Sample ID\": df_test[\"Sample ID\"].values, \"Target\": average_prediction}).sort_values(\"Sample ID\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uGJVXsOW7h-"},"outputs":[],"source":["def sanity_checks(submission):\n","    assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n","    assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n","    assert list(submission.columns) == [\"Sample ID\", \"Target\",], \"Your submission file must have columns `Sample ID` and `Target`\"\n","\n","sanity_checks(CHOWDER_submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhV0ZzImXBOC"},"outputs":[],"source":["submission_path = '/content/drive/My Drive/Breast_Cancer_Detection/Predictions/'\n","\n","CHOWDER_submission.to_csv(submission_path + \"CHOWDER_submission_hybrid.csv\", index=None)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPYG+fLTw21YgkgJxCHveRJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}