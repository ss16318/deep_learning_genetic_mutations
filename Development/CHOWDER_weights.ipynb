{"cells":[{"cell_type":"markdown","metadata":{"id":"E7a7WuSCGBiS"},"source":["# CHOWDER"]},{"cell_type":"markdown","metadata":{"id":"3WTUvo2GGELp"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13656,"status":"ok","timestamp":1697503103452,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"B7K98CEankGG","outputId":"4f505d0c-5e64-433e-b11d-a0467cc92c3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: numpy\u003e1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch\u003e=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n","Requirement already satisfied: lightning-utilities\u003e=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n","Requirement already satisfied: packaging\u003e=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities\u003e=0.8.0-\u003etorchmetrics) (23.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities\u003e=0.8.0-\u003etorchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.1-\u003etorchmetrics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.8.1-\u003etorchmetrics) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.8.1-\u003etorchmetrics) (17.0.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.8.1-\u003etorchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.8.1-\u003etorchmetrics) (1.3.0)\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14815,"status":"ok","timestamp":1697503118258,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"mRdeZmxRF6lv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchmetrics"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":760,"status":"ok","timestamp":1697503118969,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"_Ym0jJzmGMwX","outputId":"9aaf137e-ad3a-4b13-8b84-06d541b18adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"GEP9wThKGNmv"},"source":["## Load Data"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":27063,"status":"ok","timestamp":1697503146025,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"RA9sRTmAGPbH"},"outputs":[],"source":["processed_data_path = '/content/drive/My Drive/Breast_Cancer_Detection/Processed_Data/'\n","\n","X_dev = np.load(processed_data_path + 'X_dev.npy')\n","y_dev = np.load(processed_data_path + 'y_dev.npy')\n","\n","X_test = np.load(processed_data_path + 'X_test.npy')"]},{"cell_type":"markdown","metadata":{"id":"9UZ5pixlHaP_"},"source":["## Split Data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3024,"status":"ok","timestamp":1697503149000,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"VvD3AJlWHbl6"},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.15, stratify=y_dev, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"kCQyfA4KGiJT"},"source":["## Standardize Data"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1697503149001,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"Rrujd_x7GmCJ"},"outputs":[],"source":["def X_standardize(X_train, X_val, X_test):\n","\n","    feature_mean = np.mean(X_train)\n","    feature_std = np.std(X_train)\n","\n","    X_train_scaled = (X_train - feature_mean) / feature_std\n","    X_val_scaled = (X_val - feature_mean) / feature_std\n","    X_test_scaled = (X_test - feature_mean) / feature_std\n","\n","    return X_train_scaled, X_val_scaled, X_test_scaled"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":18723,"status":"ok","timestamp":1697503167705,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"5r1BHVpzHwEP"},"outputs":[],"source":["X_train_scaled, X_val_scaled, X_test_scaled = X_standardize(X_train, X_val, X_test)"]},{"cell_type":"markdown","metadata":{"id":"ypElU9wJJO2X"},"source":["## Convert to Tensor"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1300,"status":"ok","timestamp":1697503168954,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"uHP07ms3I22G"},"outputs":[],"source":["X_train_tensor = torch.Tensor(X_train_scaled)\n","X_val_tensor = torch.Tensor(X_val_scaled)\n","X_test_tensor = torch.Tensor(X_test_scaled)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1697503169089,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"4FLD-9qYJTJ9"},"outputs":[],"source":["# Delete redundant variables to free up memory\n","del X_train_scaled\n","del X_val_scaled\n","del X_test_scaled\n","del X_train\n","del X_val\n","del X_test\n","del X_dev"]},{"cell_type":"markdown","metadata":{"id":"lvkzmqLUJ9o3"},"source":["## CHOWDER Model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1697503169090,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"bs8Yoh9SJ_wP"},"outputs":[],"source":["class CHOWDER(nn.Module):\n","\n","    def __init__(self):\n","        super(CHOWDER, self).__init__()\n","\n","        # Convolutional layer\n","        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2048, stride=2048)\n","\n","        # MLP layers with dropout\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(4, 200),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(200, 100),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc3 = nn.Sequential(\n","            nn.Linear(100, 1),\n","            nn.Dropout(0.5)\n","        )\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","\n","        for layer in [self.conv, self.fc1[0], self.fc2[0], self.fc3[0]]:\n","\n","            # Calculate the number of edges in the layer\n","            if layer == self.conv:\n","                num_edges = 2048\n","            if layer == self.fc1[0]:\n","                num_edges = 800\n","            if layer == self.fc2[0]:\n","                num_edges = 20000\n","            if layer == self.fc3[0]:\n","                num_edges = 100\n","\n","            # Set std to the square root of the number of edges\n","            std = num_edges**(-0.5)\n","\n","            # Initialize weights with random normal values\n","            nn.init.normal_(layer.weight, mean=0.0, std=std)\n","            nn.init.constant_(layer.bias, 0)\n","\n","\n","\n","    def forward(self, x):\n","\n","        ## CONVOLUTION LAYER\n","        conv_output = self.conv(x)\n","\n","        # Calculate L2-norm on weights in the convolutional layer\n","        l2_reg = 0.0\n","        for param in self.conv.parameters():\n","            l2_reg += torch.sum(param ** 2)\n","\n","        ## MINMAX LAYER\n","\n","        # Sort each row of the conv layer (each row is a sample)\n","        sorted_output, _ = torch.sort(conv_output, dim=2)\n","\n","        # Number of top instances and negative evidence\n","        R = 2\n","\n","        # Select the first two and last two sorted outputs\n","        selected_output = sorted_output[:, :, :R]\n","        a0 = torch.cat((selected_output, sorted_output[:, :, -R:]), dim=2)\n","\n","        a0 = a0.squeeze()\n","\n","        ## MLP\n","\n","        # Layer 1\n","        z1 = self.fc1(a0)\n","        a1 = torch.sigmoid(z1)\n","\n","        # Layer 2\n","        z2 = self.fc2(a1)\n","        a2 = torch.sigmoid(z2)\n","\n","        # Output layer\n","        z3 = self.fc3(a2)\n","        output = torch.sigmoid(z3)\n","\n","        return output, l2_reg"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179,"status":"ok","timestamp":1697503169261,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"LfK_D8ki0_8a","outputId":"cc554b47-7327-4463-c010-a4f4828d14c1"},"outputs":[{"data":{"text/plain":["CHOWDER(\n","  (conv): Conv1d(1, 1, kernel_size=(2048,), stride=(2048,))\n","  (fc1): Sequential(\n","    (0): Linear(in_features=4, out_features=200, bias=True)\n","    (1): Dropout(p=0.5, inplace=False)\n","  )\n","  (fc2): Sequential(\n","    (0): Linear(in_features=200, out_features=100, bias=True)\n","    (1): Dropout(p=0.5, inplace=False)\n","  )\n","  (fc3): Sequential(\n","    (0): Linear(in_features=100, out_features=1, bias=True)\n","    (1): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["demo_model = CHOWDER()\n","\n","demo_model"]},{"cell_type":"markdown","metadata":{"id":"k9FI8PUMSAHW"},"source":["## Setup Hyperparameters and Data Loader"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1697503169262,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"fwA77N-8SDvp"},"outputs":[],"source":["# Define hyperparameters\n","BATCH_SIZE = 10\n","LR = 0.001\n","EPOCHS = 30\n","NUM_ENSEMBLE_MODELS = 10"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1697503169263,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"sHLHzyStTH9S"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, data_tensor, target_tensor):\n","        self.data = data_tensor\n","        self.target = target_tensor\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.target[index]\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","# Create custom datasets\n","train_dataset = CustomDataset(X_train_tensor, torch.Tensor(y_train))\n","val_dataset = CustomDataset(X_val_tensor, torch.Tensor(y_val))\n","\n","# Create DataLoader\n","train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1697503169263,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"Pusa-2_1Tbs5"},"outputs":[],"source":["# Free Up Memory\n","del X_train_tensor\n","del X_val_tensor\n","del y_train\n","del y_val"]},{"cell_type":"markdown","metadata":{"id":"d3_KACwOkeNm"},"source":["## Train Model and Validate"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1697503169392,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"_X_vyiqBkdj-"},"outputs":[],"source":["# # Define loss function and optimizer\n","# loss_function = nn.BCELoss()\n","\n","# # Create an ensemble of models\n","# ensemble_models = []\n","\n","# for _ in range(NUM_ENSEMBLE_MODELS):\n","\n","#     # Initialize the model\n","#     model = CHOWDER()\n","\n","#     # Define optimizer\n","#     optimizer = optim.Adam(model.parameters(), lr=LR)\n","#     ensemble_models.append((model, optimizer))\n","\n","# # Setup model counter\n","# model_counter = 1\n","\n","# # Training and validation loops for each model in the ensemble\n","# for model, optimizer in ensemble_models:\n","\n","#     print(f'\\n ----- Model {model_counter} -----')\n","#     model_counter += 1\n","\n","#     # Loop thorugh each epoch\n","#     for epoch in range(EPOCHS):\n","\n","#         ## Training loop\n","\n","#         # Put model in train mode\n","#         model.train()\n","\n","#         # Initialize loss, AUC and count\n","#         total_loss = 0.0\n","#         auroc_hist_train = 0.0\n","#         total_count = 0.0\n","\n","#         # Train in batches\n","#         for batch_x, batch_y in train_dl:\n","\n","#             batch_x_transformed = batch_x.unsqueeze(1)\n","\n","#             # Zero gradients\n","#             optimizer.zero_grad()\n","\n","#             # Make predictions and get L2-norm from conv layer\n","#             pred, weight_decay = model(batch_x_transformed)\n","\n","#             # Calculate loss\n","#             batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","#             loss = loss_function(pred, batch_y) + (0.5*weight_decay)\n","\n","#             # Calculate gradients\n","#             loss.backward()\n","\n","#             # Make step in gradient descent\n","#             optimizer.step()\n","\n","#             # Add to loss counter for this epoch\n","#             total_loss += loss.item() * len(batch_y)\n","#             total_count += len(batch_y)\n","\n","#             # Calculate AUC for batch\n","#             auroc_hist_train += torchmetrics.AUROC(task=\"binary\")(pred, batch_y).item() * len(batch_y)\n","\n","#         # Calculate loss and AUC per sample\n","#         train_average_loss = total_loss / total_count\n","#         train_average_auc = auroc_hist_train/ total_count\n","\n","\n","#         ## Validation loop\n","\n","#         # Put model in evaluation mode\n","#         model.eval()\n","\n","#         # Fix gradients (only using model to predict)\n","#         with torch.no_grad():\n","\n","#             # Initialize loss, AUC and count\n","#             total_loss = 0.0\n","#             auroc_hist_val = 0.0\n","#             total_count = 0.0\n","\n","#             # Validate in batches\n","#             for batch_x, batch_y in val_dl:\n","\n","#                 batch_x_transformed = batch_x.unsqueeze(1)\n","\n","#                 # Make predictions\n","#                 val_pred, _ = model(batch_x_transformed)\n","\n","#                 # Calculate loss\n","#                 batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","#                 loss = loss_function(val_pred, batch_y)\n","\n","#                 # Add to loss for this epoch\n","#                 total_loss += loss.item() * len(batch_y)\n","#                 total_count += len(batch_y)\n","\n","#                 # Calculate AUC for batch\n","#                 auroc_hist_val += torchmetrics.AUROC(task=\"binary\")(val_pred, batch_y).item() * len(batch_y)\n","\n","#         # Calculate loss and AUC per sample\n","#         val_average_loss = total_loss / total_count\n","#         val_average_auc =  auroc_hist_val / total_count\n","\n","#         # Print results from each epoch\n","#         print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_average_loss:.4f} Train AUC Score: {train_average_auc:.4f} \\\n","#         Val Loss: {val_average_loss:.4f} Val AUC Score: {val_average_auc:.4f}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1697503169393,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"kLJ3f4HD63Zb"},"outputs":[],"source":["# # After training all models, calculate the ensemble AUC\n","\n","# # Store predictions and labels\n","# ensemble_predictions = []\n","# val_labels = []\n","\n","# # Do not change gradients\n","# with torch.no_grad():\n","\n","#     # Loop through each model\n","#     for model, _ in ensemble_models:\n","\n","#         # Put model in evaluation mode\n","#         model.eval()\n","\n","#         # Get predictions on the validation dataset\n","#         predictions = []\n","#         batch_labels = []  # Create a list to store labels for each batch\n","\n","#         for batch_x, batch_y in val_dl:\n","\n","#             batch_x_transformed = batch_x.unsqueeze(1)\n","\n","#             # Make predictions\n","#             pred, _ = model(batch_x_transformed)\n","#             # Save predictions\n","#             predictions.append(pred)\n","\n","#             # Store val labels for this batch\n","#             batch_labels.append(batch_y)\n","\n","#         # Concatenate the labels for this model\n","#         batch_labels = torch.cat(batch_labels, dim=0)\n","#         val_labels.append(batch_labels)\n","\n","#         predictions = torch.cat(predictions)\n","#         ensemble_predictions.append(predictions)\n","\n","# # Concatenate all the validation labels\n","# val_labels = torch.cat(val_labels, dim=0)\n","\n","# # Average the predictions from all models\n","# ensemble_predictions = torch.stack(ensemble_predictions)\n","# average_predictions = torch.mean(ensemble_predictions, dim=0)\n","\n","# # Calculate the AUC score based on the averaged predictions\n","# average_auc = torchmetrics.AUROC(task=\"binary\")(average_predictions, val_labels).item()\n","# print(f\"Ensemble AUC Score: {average_auc:.4f}\")\n","\n","# submission_path = '/content/drive/My Drive/Breast_Cancer_Detection/Predictions/'\n","# # Save the AUC score to a CSV file as a string\n","# with open(submission_path + \"weights_val_AUC_normal.csv\", \"w\") as file:\n","#     file.write(f\"AUC Score\\n{average_auc:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"5eCI-fHc1Oeb"},"source":["## Test Model"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1697503169394,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"mU3UREXZ1QH_"},"outputs":[],"source":["## Setup DataLoaders\n","\n","# Create a dummy y for test set\n","y_test_dummy = torch.zeros(len(X_test_tensor),)\n","\n","# Create custom datasets\n","test_dataset = CustomDataset(X_test_tensor, y_test_dummy)\n","\n","# Create DataLoader\n","test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# Merge them using ConcatDataset\n","dev_dataset = ConcatDataset([train_dataset, val_dataset])\n","\n","# Create a DataLoader for the merged dataset\n","dev_dl = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1697503169394,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"uaEiv9s03HQA"},"outputs":[],"source":["# Free Up Memory\n","del X_test_tensor\n","del train_dataset\n","del val_dataset"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021494,"status":"ok","timestamp":1697505413275,"user":{"displayName":"Sebastian Samuel Steiner","userId":"17762157590054253406"},"user_tz":240},"id":"WcMwUt273UCM"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," ----- Model 1 -----\n","Epoch [1/30] - Train Loss: 0.8807 Train AUC Score: 0.5031\n","Epoch [2/30] - Train Loss: 0.6901 Train AUC Score: 0.5165\n","Epoch [3/30] - Train Loss: 0.6790 Train AUC Score: 0.5569\n","Epoch [4/30] - Train Loss: 0.6771 Train AUC Score: 0.5310\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/30] - Train Loss: 0.6806 Train AUC Score: 0.4933\n","Epoch [6/30] - Train Loss: 0.6601 Train AUC Score: 0.5421\n","Epoch [7/30] - Train Loss: 0.6909 Train AUC Score: 0.4957\n","Epoch [8/30] - Train Loss: 0.6906 Train AUC Score: 0.5062\n","Epoch [9/30] - Train Loss: 0.7053 Train AUC Score: 0.4684\n","Epoch [10/30] - Train Loss: 0.7024 Train AUC Score: 0.4767\n","Epoch [11/30] - Train Loss: 0.6526 Train AUC Score: 0.5748\n","Epoch [12/30] - Train Loss: 0.6919 Train AUC Score: 0.5627\n","Epoch [13/30] - Train Loss: 0.6670 Train AUC Score: 0.5256\n","Epoch [14/30] - Train Loss: 0.6789 Train AUC Score: 0.5144\n","Epoch [15/30] - Train Loss: 0.6986 Train AUC Score: 0.4906\n","Epoch [16/30] - Train Loss: 0.6973 Train AUC Score: 0.5120\n","Epoch [17/30] - Train Loss: 0.6850 Train AUC Score: 0.4824\n","Epoch [18/30] - Train Loss: 0.6780 Train AUC Score: 0.5397\n","Epoch [19/30] - Train Loss: 0.6868 Train AUC Score: 0.4757\n","Epoch [20/30] - Train Loss: 0.6980 Train AUC Score: 0.4510\n","Epoch [21/30] - Train Loss: 0.6909 Train AUC Score: 0.4882\n","Epoch [22/30] - Train Loss: 0.7004 Train AUC Score: 0.4510\n","Epoch [23/30] - Train Loss: 0.7018 Train AUC Score: 0.4273\n","Epoch [24/30] - Train Loss: 0.7074 Train AUC Score: 0.3956\n","Epoch [25/30] - Train Loss: 0.6879 Train AUC Score: 0.4809\n","Epoch [26/30] - Train Loss: 0.6960 Train AUC Score: 0.4620\n","Epoch [27/30] - Train Loss: 0.6738 Train AUC Score: 0.5153\n","Epoch [28/30] - Train Loss: 0.6750 Train AUC Score: 0.5223\n","Epoch [29/30] - Train Loss: 0.6815 Train AUC Score: 0.5555\n","Epoch [30/30] - Train Loss: 0.6845 Train AUC Score: 0.4947\n","\n"," ----- Model 2 -----\n","Epoch [1/30] - Train Loss: 0.8740 Train AUC Score: 0.4899\n","Epoch [2/30] - Train Loss: 0.6925 Train AUC Score: 0.5226\n","Epoch [3/30] - Train Loss: 0.6970 Train AUC Score: 0.4729\n","Epoch [4/30] - Train Loss: 0.6836 Train AUC Score: 0.4928\n","Epoch [5/30] - Train Loss: 0.6867 Train AUC Score: 0.4787\n","Epoch [6/30] - Train Loss: 0.6872 Train AUC Score: 0.4741\n","Epoch [7/30] - Train Loss: 0.6904 Train AUC Score: 0.4395\n","Epoch [8/30] - Train Loss: 0.6866 Train AUC Score: 0.4968\n","Epoch [9/30] - Train Loss: 0.7226 Train AUC Score: 0.4171\n","Epoch [10/30] - Train Loss: 0.6831 Train AUC Score: 0.5066\n","Epoch [11/30] - Train Loss: 0.6902 Train AUC Score: 0.4869\n","Epoch [12/30] - Train Loss: 0.7067 Train AUC Score: 0.4731\n","Epoch [13/30] - Train Loss: 0.7094 Train AUC Score: 0.4812\n","Epoch [14/30] - Train Loss: 0.6828 Train AUC Score: 0.5207\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n","  warnings.warn(*args, **kwargs)  # noqa: B028\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [15/30] - Train Loss: 0.6802 Train AUC Score: 0.5177\n","Epoch [16/30] - Train Loss: 0.6599 Train AUC Score: 0.5683\n","Epoch [17/30] - Train Loss: 0.6957 Train AUC Score: 0.4718\n","Epoch [18/30] - Train Loss: 0.7101 Train AUC Score: 0.4780\n","Epoch [19/30] - Train Loss: 0.6837 Train AUC Score: 0.5022\n","Epoch [20/30] - Train Loss: 0.6919 Train AUC Score: 0.4745\n","Epoch [21/30] - Train Loss: 0.6996 Train AUC Score: 0.4793\n","Epoch [22/30] - Train Loss: 0.6983 Train AUC Score: 0.4515\n","Epoch [23/30] - Train Loss: 0.6805 Train AUC Score: 0.5298\n","Epoch [24/30] - Train Loss: 0.6876 Train AUC Score: 0.4674\n","Epoch [25/30] - Train Loss: 0.6829 Train AUC Score: 0.4909\n","Epoch [26/30] - Train Loss: 0.6855 Train AUC Score: 0.4846\n","Epoch [27/30] - Train Loss: 0.6976 Train AUC Score: 0.4807\n","Epoch [28/30] - Train Loss: 0.6868 Train AUC Score: 0.5130\n","Epoch [29/30] - Train Loss: 0.6905 Train AUC Score: 0.4738\n","Epoch [30/30] - Train Loss: 0.6719 Train AUC Score: 0.5240\n","\n"," ----- Model 3 -----\n","Epoch [1/30] - Train Loss: 0.9215 Train AUC Score: 0.4209\n","Epoch [2/30] - Train Loss: 0.6994 Train AUC Score: 0.4920\n","Epoch [3/30] - Train Loss: 0.6704 Train AUC Score: 0.5247\n","Epoch [4/30] - Train Loss: 0.6911 Train AUC Score: 0.5250\n","Epoch [5/30] - Train Loss: 0.6803 Train AUC Score: 0.4985\n","Epoch [6/30] - Train Loss: 0.6848 Train AUC Score: 0.4829\n","Epoch [7/30] - Train Loss: 0.6869 Train AUC Score: 0.4910\n","Epoch [8/30] - Train Loss: 0.6695 Train AUC Score: 0.5545\n","Epoch [9/30] - Train Loss: 0.6639 Train AUC Score: 0.5563\n","Epoch [10/30] - Train Loss: 0.6797 Train AUC Score: 0.4985\n","Epoch [11/30] - Train Loss: 0.6838 Train AUC Score: 0.4475\n","Epoch [12/30] - Train Loss: 0.7186 Train AUC Score: 0.4247\n","Epoch [13/30] - Train Loss: 0.6877 Train AUC Score: 0.4889\n","Epoch [14/30] - Train Loss: 0.6904 Train AUC Score: 0.5083\n","Epoch [15/30] - Train Loss: 0.7027 Train AUC Score: 0.4389\n","Epoch [16/30] - Train Loss: 0.6824 Train AUC Score: 0.5037\n","Epoch [17/30] - Train Loss: 0.6653 Train AUC Score: 0.5314\n","Epoch [18/30] - Train Loss: 0.6907 Train AUC Score: 0.4749\n","Epoch [19/30] - Train Loss: 0.6982 Train AUC Score: 0.5441\n","Epoch [20/30] - Train Loss: 0.6760 Train AUC Score: 0.5157\n","Epoch [21/30] - Train Loss: 0.6806 Train AUC Score: 0.5183\n","Epoch [22/30] - Train Loss: 0.7026 Train AUC Score: 0.4470\n","Epoch [23/30] - Train Loss: 0.6866 Train AUC Score: 0.5126\n","Epoch [24/30] - Train Loss: 0.6679 Train AUC Score: 0.5484\n","Epoch [25/30] - Train Loss: 0.6924 Train AUC Score: 0.4982\n","Epoch [26/30] - Train Loss: 0.6730 Train AUC Score: 0.5150\n","Epoch [27/30] - Train Loss: 0.6826 Train AUC Score: 0.5127\n","Epoch [28/30] - Train Loss: 0.6961 Train AUC Score: 0.4762\n","Epoch [29/30] - Train Loss: 0.6823 Train AUC Score: 0.5584\n","Epoch [30/30] - Train Loss: 0.6849 Train AUC Score: 0.4312\n","\n"," ----- Model 4 -----\n","Epoch [1/30] - Train Loss: 0.9010 Train AUC Score: 0.5227\n","Epoch [2/30] - Train Loss: 0.7004 Train AUC Score: 0.5127\n","Epoch [3/30] - Train Loss: 0.6769 Train AUC Score: 0.5198\n","Epoch [4/30] - Train Loss: 0.6926 Train AUC Score: 0.4491\n","Epoch [5/30] - Train Loss: 0.6776 Train AUC Score: 0.4989\n","Epoch [6/30] - Train Loss: 0.6868 Train AUC Score: 0.4711\n","Epoch [7/30] - Train Loss: 0.6941 Train AUC Score: 0.4793\n","Epoch [8/30] - Train Loss: 0.6883 Train AUC Score: 0.4970\n","Epoch [9/30] - Train Loss: 0.6709 Train AUC Score: 0.5621\n","Epoch [10/30] - Train Loss: 0.6825 Train AUC Score: 0.5186\n","Epoch [11/30] - Train Loss: 0.6901 Train AUC Score: 0.4768\n","Epoch [12/30] - Train Loss: 0.6922 Train AUC Score: 0.4714\n","Epoch [13/30] - Train Loss: 0.6662 Train AUC Score: 0.5496\n","Epoch [14/30] - Train Loss: 0.6888 Train AUC Score: 0.4797\n","Epoch [15/30] - Train Loss: 0.6719 Train AUC Score: 0.5081\n","Epoch [16/30] - Train Loss: 0.6875 Train AUC Score: 0.5027\n","Epoch [17/30] - Train Loss: 0.6827 Train AUC Score: 0.4752\n","Epoch [18/30] - Train Loss: 0.6707 Train AUC Score: 0.5381\n","Epoch [19/30] - Train Loss: 0.6790 Train AUC Score: 0.5053\n","Epoch [20/30] - Train Loss: 0.7060 Train AUC Score: 0.4992\n","Epoch [21/30] - Train Loss: 0.6631 Train AUC Score: 0.5769\n","Epoch [22/30] - Train Loss: 0.6795 Train AUC Score: 0.4658\n","Epoch [23/30] - Train Loss: 0.6872 Train AUC Score: 0.5096\n","Epoch [24/30] - Train Loss: 0.6912 Train AUC Score: 0.5063\n","Epoch [25/30] - Train Loss: 0.6932 Train AUC Score: 0.4972\n","Epoch [26/30] - Train Loss: 0.6800 Train AUC Score: 0.5010\n","Epoch [27/30] - Train Loss: 0.6771 Train AUC Score: 0.5318\n","Epoch [28/30] - Train Loss: 0.6842 Train AUC Score: 0.4808\n","Epoch [29/30] - Train Loss: 0.6719 Train AUC Score: 0.5595\n","Epoch [30/30] - Train Loss: 0.6972 Train AUC Score: 0.4379\n","\n"," ----- Model 5 -----\n","Epoch [1/30] - Train Loss: 0.8993 Train AUC Score: 0.5168\n","Epoch [2/30] - Train Loss: 0.7191 Train AUC Score: 0.4550\n","Epoch [3/30] - Train Loss: 0.6932 Train AUC Score: 0.4140\n","Epoch [4/30] - Train Loss: 0.6889 Train AUC Score: 0.4992\n","Epoch [5/30] - Train Loss: 0.6962 Train AUC Score: 0.4283\n","Epoch [6/30] - Train Loss: 0.7070 Train AUC Score: 0.4884\n","Epoch [7/30] - Train Loss: 0.6988 Train AUC Score: 0.5459\n","Epoch [8/30] - Train Loss: 0.6685 Train AUC Score: 0.5457\n","Epoch [9/30] - Train Loss: 0.6885 Train AUC Score: 0.4942\n","Epoch [10/30] - Train Loss: 0.6773 Train AUC Score: 0.5231\n","Epoch [11/30] - Train Loss: 0.6789 Train AUC Score: 0.5480\n","Epoch [12/30] - Train Loss: 0.6864 Train AUC Score: 0.5124\n","Epoch [13/30] - Train Loss: 0.6815 Train AUC Score: 0.5306\n","Epoch [14/30] - Train Loss: 0.6783 Train AUC Score: 0.5275\n","Epoch [15/30] - Train Loss: 0.6739 Train AUC Score: 0.5311\n","Epoch [16/30] - Train Loss: 0.6758 Train AUC Score: 0.5320\n","Epoch [17/30] - Train Loss: 0.6747 Train AUC Score: 0.5309\n","Epoch [18/30] - Train Loss: 0.6921 Train AUC Score: 0.4943\n","Epoch [19/30] - Train Loss: 0.6801 Train AUC Score: 0.4821\n","Epoch [20/30] - Train Loss: 0.7017 Train AUC Score: 0.4498\n","Epoch [21/30] - Train Loss: 0.6923 Train AUC Score: 0.4628\n","Epoch [22/30] - Train Loss: 0.7034 Train AUC Score: 0.4343\n","Epoch [23/30] - Train Loss: 0.6874 Train AUC Score: 0.4945\n","Epoch [24/30] - Train Loss: 0.6831 Train AUC Score: 0.4601\n","Epoch [25/30] - Train Loss: 0.6902 Train AUC Score: 0.4876\n","Epoch [26/30] - Train Loss: 0.6963 Train AUC Score: 0.5411\n","Epoch [27/30] - Train Loss: 0.6659 Train AUC Score: 0.5626\n","Epoch [28/30] - Train Loss: 0.6936 Train AUC Score: 0.4874\n","Epoch [29/30] - Train Loss: 0.6845 Train AUC Score: 0.5298\n","Epoch [30/30] - Train Loss: 0.6862 Train AUC Score: 0.4890\n","\n"," ----- Model 6 -----\n","Epoch [1/30] - Train Loss: 0.8913 Train AUC Score: 0.5369\n","Epoch [2/30] - Train Loss: 0.6972 Train AUC Score: 0.5021\n","Epoch [3/30] - Train Loss: 0.6809 Train AUC Score: 0.5254\n","Epoch [4/30] - Train Loss: 0.6827 Train AUC Score: 0.5211\n","Epoch [5/30] - Train Loss: 0.6695 Train AUC Score: 0.5806\n","Epoch [6/30] - Train Loss: 0.6778 Train AUC Score: 0.5296\n","Epoch [7/30] - Train Loss: 0.6858 Train AUC Score: 0.4848\n","Epoch [8/30] - Train Loss: 0.6685 Train AUC Score: 0.5204\n","Epoch [9/30] - Train Loss: 0.6776 Train AUC Score: 0.4997\n","Epoch [10/30] - Train Loss: 0.6893 Train AUC Score: 0.4740\n","Epoch [11/30] - Train Loss: 0.6825 Train AUC Score: 0.4764\n","Epoch [12/30] - Train Loss: 0.6831 Train AUC Score: 0.5280\n","Epoch [13/30] - Train Loss: 0.6853 Train AUC Score: 0.4752\n","Epoch [14/30] - Train Loss: 0.6940 Train AUC Score: 0.4799\n","Epoch [15/30] - Train Loss: 0.6813 Train AUC Score: 0.4994\n","Epoch [16/30] - Train Loss: 0.6948 Train AUC Score: 0.5017\n","Epoch [17/30] - Train Loss: 0.7057 Train AUC Score: 0.4914\n","Epoch [18/30] - Train Loss: 0.6952 Train AUC Score: 0.4677\n","Epoch [19/30] - Train Loss: 0.6738 Train AUC Score: 0.5127\n","Epoch [20/30] - Train Loss: 0.6803 Train AUC Score: 0.4871\n","Epoch [21/30] - Train Loss: 0.6874 Train AUC Score: 0.5080\n","Epoch [22/30] - Train Loss: 0.7019 Train AUC Score: 0.4603\n","Epoch [23/30] - Train Loss: 0.6852 Train AUC Score: 0.4991\n","Epoch [24/30] - Train Loss: 0.6699 Train AUC Score: 0.5497\n","Epoch [25/30] - Train Loss: 0.6795 Train AUC Score: 0.4727\n","Epoch [26/30] - Train Loss: 0.7045 Train AUC Score: 0.4720\n","Epoch [27/30] - Train Loss: 0.7096 Train AUC Score: 0.4409\n","Epoch [28/30] - Train Loss: 0.6935 Train AUC Score: 0.4787\n","Epoch [29/30] - Train Loss: 0.6794 Train AUC Score: 0.4977\n","Epoch [30/30] - Train Loss: 0.6800 Train AUC Score: 0.4910\n","\n"," ----- Model 7 -----\n","Epoch [1/30] - Train Loss: 0.9202 Train AUC Score: 0.4584\n","Epoch [2/30] - Train Loss: 0.7079 Train AUC Score: 0.4689\n","Epoch [3/30] - Train Loss: 0.6993 Train AUC Score: 0.4487\n","Epoch [4/30] - Train Loss: 0.6938 Train AUC Score: 0.4479\n","Epoch [5/30] - Train Loss: 0.6824 Train AUC Score: 0.5014\n","Epoch [6/30] - Train Loss: 0.6912 Train AUC Score: 0.4312\n","Epoch [7/30] - Train Loss: 0.6889 Train AUC Score: 0.4746\n","Epoch [8/30] - Train Loss: 0.6886 Train AUC Score: 0.4574\n","Epoch [9/30] - Train Loss: 0.6632 Train AUC Score: 0.5479\n","Epoch [10/30] - Train Loss: 0.6983 Train AUC Score: 0.4671\n","Epoch [11/30] - Train Loss: 0.6813 Train AUC Score: 0.5005\n","Epoch [12/30] - Train Loss: 0.6941 Train AUC Score: 0.4463\n","Epoch [13/30] - Train Loss: 0.7085 Train AUC Score: 0.4361\n","Epoch [14/30] - Train Loss: 0.6860 Train AUC Score: 0.4690\n","Epoch [15/30] - Train Loss: 0.6882 Train AUC Score: 0.4654\n","Epoch [16/30] - Train Loss: 0.6965 Train AUC Score: 0.4299\n","Epoch [17/30] - Train Loss: 0.6775 Train AUC Score: 0.4956\n","Epoch [18/30] - Train Loss: 0.6821 Train AUC Score: 0.4979\n","Epoch [19/30] - Train Loss: 0.6921 Train AUC Score: 0.4726\n","Epoch [20/30] - Train Loss: 0.6623 Train AUC Score: 0.5596\n","Epoch [21/30] - Train Loss: 0.6788 Train AUC Score: 0.5374\n","Epoch [22/30] - Train Loss: 0.6943 Train AUC Score: 0.4441\n","Epoch [23/30] - Train Loss: 0.6722 Train AUC Score: 0.5766\n","Epoch [24/30] - Train Loss: 0.6725 Train AUC Score: 0.5164\n","Epoch [25/30] - Train Loss: 0.6968 Train AUC Score: 0.4584\n","Epoch [26/30] - Train Loss: 0.7059 Train AUC Score: 0.4300\n","Epoch [27/30] - Train Loss: 0.6762 Train AUC Score: 0.4941\n","Epoch [28/30] - Train Loss: 0.6954 Train AUC Score: 0.4530\n","Epoch [29/30] - Train Loss: 0.6863 Train AUC Score: 0.4616\n","Epoch [30/30] - Train Loss: 0.6946 Train AUC Score: 0.4701\n","\n"," ----- Model 8 -----\n","Epoch [1/30] - Train Loss: 0.9096 Train AUC Score: 0.4917\n","Epoch [2/30] - Train Loss: 0.6833 Train AUC Score: 0.5617\n","Epoch [3/30] - Train Loss: 0.6742 Train AUC Score: 0.5001\n","Epoch [4/30] - Train Loss: 0.6951 Train AUC Score: 0.4166\n","Epoch [5/30] - Train Loss: 0.6670 Train AUC Score: 0.5836\n","Epoch [6/30] - Train Loss: 0.6859 Train AUC Score: 0.4973\n","Epoch [7/30] - Train Loss: 0.6881 Train AUC Score: 0.5391\n","Epoch [8/30] - Train Loss: 0.6719 Train AUC Score: 0.5143\n","Epoch [9/30] - Train Loss: 0.6991 Train AUC Score: 0.4780\n","Epoch [10/30] - Train Loss: 0.6668 Train AUC Score: 0.5603\n","Epoch [11/30] - Train Loss: 0.6747 Train AUC Score: 0.5612\n","Epoch [12/30] - Train Loss: 0.6845 Train AUC Score: 0.5116\n","Epoch [13/30] - Train Loss: 0.6936 Train AUC Score: 0.4672\n","Epoch [14/30] - Train Loss: 0.6810 Train AUC Score: 0.4987\n","Epoch [15/30] - Train Loss: 0.6870 Train AUC Score: 0.4511\n","Epoch [16/30] - Train Loss: 0.6875 Train AUC Score: 0.5175\n","Epoch [17/30] - Train Loss: 0.6665 Train AUC Score: 0.5327\n","Epoch [18/30] - Train Loss: 0.6865 Train AUC Score: 0.4648\n","Epoch [19/30] - Train Loss: 0.6733 Train AUC Score: 0.5157\n","Epoch [20/30] - Train Loss: 0.6756 Train AUC Score: 0.5221\n","Epoch [21/30] - Train Loss: 0.6674 Train AUC Score: 0.5535\n","Epoch [22/30] - Train Loss: 0.6916 Train AUC Score: 0.5167\n","Epoch [23/30] - Train Loss: 0.7020 Train AUC Score: 0.4569\n","Epoch [24/30] - Train Loss: 0.6919 Train AUC Score: 0.4850\n","Epoch [25/30] - Train Loss: 0.6720 Train AUC Score: 0.5331\n","Epoch [26/30] - Train Loss: 0.6617 Train AUC Score: 0.5269\n","Epoch [27/30] - Train Loss: 0.6872 Train AUC Score: 0.4889\n","Epoch [28/30] - Train Loss: 0.6935 Train AUC Score: 0.4266\n","Epoch [29/30] - Train Loss: 0.6774 Train AUC Score: 0.5022\n","Epoch [30/30] - Train Loss: 0.6768 Train AUC Score: 0.5155\n","\n"," ----- Model 9 -----\n","Epoch [1/30] - Train Loss: 0.9146 Train AUC Score: 0.5198\n","Epoch [2/30] - Train Loss: 0.7013 Train AUC Score: 0.4735\n","Epoch [3/30] - Train Loss: 0.6890 Train AUC Score: 0.4975\n","Epoch [4/30] - Train Loss: 0.6889 Train AUC Score: 0.4725\n","Epoch [5/30] - Train Loss: 0.7023 Train AUC Score: 0.4882\n","Epoch [6/30] - Train Loss: 0.6898 Train AUC Score: 0.4753\n","Epoch [7/30] - Train Loss: 0.7008 Train AUC Score: 0.4351\n","Epoch [8/30] - Train Loss: 0.7032 Train AUC Score: 0.3781\n","Epoch [9/30] - Train Loss: 0.7090 Train AUC Score: 0.4948\n","Epoch [10/30] - Train Loss: 0.6729 Train AUC Score: 0.5444\n","Epoch [11/30] - Train Loss: 0.6818 Train AUC Score: 0.4884\n","Epoch [12/30] - Train Loss: 0.6883 Train AUC Score: 0.4817\n","Epoch [13/30] - Train Loss: 0.6803 Train AUC Score: 0.4951\n","Epoch [14/30] - Train Loss: 0.7037 Train AUC Score: 0.4323\n","Epoch [15/30] - Train Loss: 0.6761 Train AUC Score: 0.5189\n","Epoch [16/30] - Train Loss: 0.6863 Train AUC Score: 0.5209\n","Epoch [17/30] - Train Loss: 0.6643 Train AUC Score: 0.5626\n","Epoch [18/30] - Train Loss: 0.6892 Train AUC Score: 0.5108\n","Epoch [19/30] - Train Loss: 0.6936 Train AUC Score: 0.4935\n","Epoch [20/30] - Train Loss: 0.6565 Train AUC Score: 0.6079\n","Epoch [21/30] - Train Loss: 0.6985 Train AUC Score: 0.5042\n","Epoch [22/30] - Train Loss: 0.7002 Train AUC Score: 0.4425\n","Epoch [23/30] - Train Loss: 0.6711 Train AUC Score: 0.5642\n","Epoch [24/30] - Train Loss: 0.6852 Train AUC Score: 0.5143\n","Epoch [25/30] - Train Loss: 0.6847 Train AUC Score: 0.4770\n","Epoch [26/30] - Train Loss: 0.6851 Train AUC Score: 0.4674\n","Epoch [27/30] - Train Loss: 0.7009 Train AUC Score: 0.4756\n","Epoch [28/30] - Train Loss: 0.6854 Train AUC Score: 0.4688\n","Epoch [29/30] - Train Loss: 0.6825 Train AUC Score: 0.5179\n","Epoch [30/30] - Train Loss: 0.6951 Train AUC Score: 0.5360\n","\n"," ----- Model 10 -----\n","Epoch [1/30] - Train Loss: 0.8967 Train AUC Score: 0.4858\n","Epoch [2/30] - Train Loss: 0.7075 Train AUC Score: 0.4822\n","Epoch [3/30] - Train Loss: 0.6941 Train AUC Score: 0.4348\n","Epoch [4/30] - Train Loss: 0.6774 Train AUC Score: 0.5089\n","Epoch [5/30] - Train Loss: 0.7061 Train AUC Score: 0.4667\n","Epoch [6/30] - Train Loss: 0.6890 Train AUC Score: 0.5058\n","Epoch [7/30] - Train Loss: 0.6708 Train AUC Score: 0.5807\n","Epoch [8/30] - Train Loss: 0.6871 Train AUC Score: 0.4857\n","Epoch [9/30] - Train Loss: 0.6601 Train AUC Score: 0.5488\n","Epoch [10/30] - Train Loss: 0.6721 Train AUC Score: 0.5472\n","Epoch [11/30] - Train Loss: 0.6958 Train AUC Score: 0.4838\n","Epoch [12/30] - Train Loss: 0.6521 Train AUC Score: 0.5919\n","Epoch [13/30] - Train Loss: 0.7217 Train AUC Score: 0.4068\n","Epoch [14/30] - Train Loss: 0.6911 Train AUC Score: 0.4949\n","Epoch [15/30] - Train Loss: 0.6704 Train AUC Score: 0.5249\n","Epoch [16/30] - Train Loss: 0.7089 Train AUC Score: 0.4311\n","Epoch [17/30] - Train Loss: 0.6722 Train AUC Score: 0.5297\n","Epoch [18/30] - Train Loss: 0.6781 Train AUC Score: 0.5374\n","Epoch [19/30] - Train Loss: 0.6846 Train AUC Score: 0.5151\n","Epoch [20/30] - Train Loss: 0.6867 Train AUC Score: 0.4796\n","Epoch [21/30] - Train Loss: 0.6752 Train AUC Score: 0.5108\n","Epoch [22/30] - Train Loss: 0.6759 Train AUC Score: 0.4821\n","Epoch [23/30] - Train Loss: 0.6767 Train AUC Score: 0.5104\n","Epoch [24/30] - Train Loss: 0.6776 Train AUC Score: 0.5129\n","Epoch [25/30] - Train Loss: 0.6652 Train AUC Score: 0.5366\n","Epoch [26/30] - Train Loss: 0.6983 Train AUC Score: 0.4046\n","Epoch [27/30] - Train Loss: 0.6821 Train AUC Score: 0.5262\n","Epoch [28/30] - Train Loss: 0.6895 Train AUC Score: 0.5269\n","Epoch [29/30] - Train Loss: 0.6839 Train AUC Score: 0.5226\n","Epoch [30/30] - Train Loss: 0.6856 Train AUC Score: 0.4947\n"]}],"source":["# Define loss function and optimizer\n","loss_function = nn.BCELoss()\n","\n","# Create an ensemble of models\n","ensemble_models = []\n","\n","for _ in range(NUM_ENSEMBLE_MODELS):\n","\n","    # Initialize the model\n","    model = CHOWDER()\n","\n","    # Define optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","    ensemble_models.append((model, optimizer))\n","\n","# Setup model counter\n","model_counter = 1\n","\n","# Training loops for each model in the ensemble\n","for model, optimizer in ensemble_models:\n","\n","    print(f'\\n ----- Model {model_counter} -----')\n","    model_counter += 1\n","\n","    # Loop thorugh each epoch\n","    for epoch in range(EPOCHS):\n","\n","        ## Training loop\n","\n","        # Put model in train mode\n","        model.train()\n","\n","        # Initialize loss, AUC and count\n","        total_loss = 0.0\n","        auroc_hist_train = 0.0\n","        total_count = 0.0\n","\n","        # Train in batches\n","        for batch_x, batch_y in dev_dl:\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # Make predictions and get L2-norm from conv layer\n","            pred, L2_term = model(batch_x_transformed)\n","\n","            # Calculate loss\n","            batch_y = batch_y.view(-1, 1) # Reshape batch_y from (10) to (10,1)\n","            loss = loss_function(pred, batch_y) + (0.5*L2_term)\n","\n","            # Calculate gradients\n","            loss.backward()\n","\n","            # Make step in gradient descent\n","            optimizer.step()\n","\n","            # Add to loss counter for this epoch\n","            total_loss += loss.item() * len(batch_y)\n","            total_count += len(batch_y)\n","\n","            # Calculate AUC for batch\n","            auroc_hist_train += torchmetrics.AUROC(task=\"binary\")(pred, batch_y).item() * len(batch_y)\n","\n","        # Calculate loss and AUC per sample\n","        train_average_loss = total_loss / total_count\n","        train_average_auc = auroc_hist_train/ total_count\n","\n","        # Print results from each epoch\n","        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_average_loss:.4f} Train AUC Score: {train_average_auc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLCvU2N73s6M"},"outputs":[],"source":["# After training all models, calculate the ensemble AUC\n","\n","# Store predictions and labels\n","ensemble_predictions = np.zeros((len(test_dataset), len(ensemble_models)))\n","\n","# Do not change gradients\n","with torch.no_grad():\n","\n","    model_counter = 0\n","\n","    # Loop through each model\n","    for model, _ in ensemble_models:\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","\n","        # Get predictions on the validation dataset\n","        predictions = np.empty((0, 1))\n","\n","        for batch_x, _ in test_dl:\n","\n","            extra = 10 - len(batch_x)\n","\n","            # If batch size is smaller than 10, pad rows in batch_x with 0s\n","            if extra \u003e 0:\n","                pad_tensor = torch.zeros((extra,) + batch_x.shape[1:], dtype=batch_x.dtype)\n","                batch_x = torch.cat((batch_x, pad_tensor), dim=0)\n","\n","            # Add a dimension for channel numbers\n","            batch_x_transformed = batch_x.unsqueeze(1)\n","\n","            # Make predictions\n","            pred, _ = model(batch_x_transformed)\n","\n","            # Save predictions\n","            pred_numpy = pred.numpy()\n","            predictions = np.concatenate((predictions, pred_numpy), axis=0)\n","\n","            if extra \u003e 0:\n","              predictions = predictions[:-extra]\n","\n","        ensemble_predictions[:, model_counter] = predictions.squeeze()\n","        model_counter += 1\n","\n","# Average the predictions from all models\n","average_prediction = np.mean(ensemble_predictions, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULTV1bdoWvXq"},"outputs":[],"source":["# Load metadata about each sample\n","data_path = '/content/drive/My Drive/Breast_Cancer_Detection/Data/'\n","df_test = pd.read_csv(data_path + \"test_metadata.csv\")\n","\n","# Join sample ID metadata with probability prediction\n","CHOWDER_submission = pd.DataFrame( {\"Sample ID\": df_test[\"Sample ID\"].values, \"Target\": average_prediction}).sort_values(\"Sample ID\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uGJVXsOW7h-"},"outputs":[],"source":["def sanity_checks(submission):\n","    assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n","    assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n","    assert list(submission.columns) == [\"Sample ID\", \"Target\",], \"Your submission file must have columns `Sample ID` and `Target`\"\n","\n","sanity_checks(CHOWDER_submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhV0ZzImXBOC"},"outputs":[],"source":["submission_path = '/content/drive/My Drive/Breast_Cancer_Detection/Predictions/'\n","\n","CHOWDER_submission.to_csv(submission_path + \"CHOWDER_submission_weights.csv\", index=None)"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPt3Jvin+5pog0CoYLC0vX6","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}